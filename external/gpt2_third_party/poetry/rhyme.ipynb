{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_globals = runpy.run_path(\"../run_generation.py\")\n",
    "sample_sequence = file_globals['sample_sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_encoder = runpy.run_path(\"../yt_encoder.py\")\n",
    "YTEncoder = yt_encoder['YTEncoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 09:48:13 - INFO - transformers.configuration_utils -   loading configuration file ../gpt2/medium/poetry/config.json\n",
      "12/22/2019 09:48:13 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "12/22/2019 09:48:13 - INFO - transformers.modeling_utils -   loading weights file ../gpt2/medium/poetry/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import threading\n",
    "import regex as re\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"rest.log\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from os import environ\n",
    "device = environ.get('DEVICE', 'cuda:0')\n",
    "\n",
    "model_path = '../gpt2/medium'\n",
    "\n",
    "tokenizer = YTEncoder.from_pretrained(model_path)\n",
    "\n",
    "poetry_model = GPT2LMHeadModel.from_pretrained(model_path + '/poetry')\n",
    "poetry_model.to(device)\n",
    "poetry_model.eval()\n",
    "\n",
    "def get_sample(model, prompt, length:int, num_samples:int, allow_linebreak:bool):\n",
    "    logger.info(\"*\" * 200)\n",
    "    logger.info(prompt)\n",
    "   \n",
    "    filter_n = tokenizer.encode('\\n')[-1:]\n",
    "    filter_single = [1] \n",
    "    filter_single += [] if allow_linebreak else filter_n\n",
    "\n",
    "    context_tokens = tokenizer.encode(prompt)\n",
    "    out = sample_sequence(\n",
    "        model=model,\n",
    "        context=context_tokens,\n",
    "        length=length,\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=0.9,\n",
    "        device=device,\n",
    "        filter_single=filter_single,\n",
    "        filter_double=filter_n,\n",
    "        num_samples=num_samples,\n",
    "    ).to('cpu')\n",
    "\n",
    "    prompt = tokenizer.decode(context_tokens)\n",
    "    len_prompt = len(prompt)\n",
    "   \n",
    "    replies = [out[item, :].tolist() for item in range(len(out))]\n",
    "    text = [tokenizer.decode(item)[len_prompt:] for item in replies]\n",
    "    reg_text = [re.match(r'[\\w\\W]*[\\.!?]\\n', item) for item in text]\n",
    "    result = [reg_item[0] if reg_item else item  for reg_item, item in zip(reg_text,text)]\n",
    "    logger.info(\"=\" * 200)\n",
    "    logger.info(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rupo.api import Engine\n",
    "from rupo.rhymes.rhymes import Rhymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = Engine(language=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   vocab = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /home/u/.anaconda/envs/gpt/lib/python3.7/site-packages/russ/models/ru-main/vocabulary.\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.registrable -   instantiating registered subclass base of <class 'allennlp.models.model.Model'>\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   type = default\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.registrable -   instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "12/22/2019 09:48:27 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /home/u/.anaconda/envs/gpt/lib/python3.7/site-packages/russ/models/ru-main/vocabulary.\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'dense_dim': 128, 'embedder': {'token_embedders': {'tokens': {'embedding_dim': 32, 'type': 'embedding'}}, 'type': 'basic'}, 'embeddings_dropout': 0.3, 'encoder': {'bidirectional': True, 'hidden_size': 128, 'input_size': 32, 'num_layers': 2, 'type': 'lstm'}, 'encoder_dropout': 0.3, 'type': 'base'} and extras {'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.type = base\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'russ.stress.models.base.BaseModel'> from params {'dense_dim': 128, 'embedder': {'token_embedders': {'tokens': {'embedding_dim': 32, 'type': 'embedding'}}, 'type': 'basic'}, 'embeddings_dropout': 0.3, 'encoder': {'bidirectional': True, 'hidden_size': 128, 'input_size': 32, 'num_layers': 2, 'type': 'lstm'}, 'encoder_dropout': 0.3} and extras {'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 32, 'type': 'embedding'}}, 'type': 'basic'} and extras {'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.type = basic\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.embedder_to_indexer_map = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.allow_unmatched_keys = False\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 32, 'type': 'embedding'} and extras {'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.type = embedding\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.num_embeddings = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.embedding_dim = 32\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.pretrained_file = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.projection_dim = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.trainable = True\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.padding_index = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.max_norm = None\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.norm_type = 2.0\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embedder.token_embedders.tokens.sparse = False\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'hidden_size': 128, 'input_size': 32, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.hidden_size = 128\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.input_size = 32\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.embeddings_dropout = 0.3\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.encoder_dropout = 0.3\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   model.dense_dim = 128\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'russ.stress.model.StressModel'> from params {'reader': {'type': 'stress'}} and extras {'stress_dict', 'model', 'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'type': 'stress'} and extras {'stress_dict', 'model', 'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   reader.type = stress\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.from_params -   instantiating class <class 'russ.stress.reader.StressReader'> from params {} and extras {'stress_dict', 'model', 'vocab'}\n",
      "12/22/2019 09:48:27 - INFO - allennlp.common.params -   reader.token_indexers = <allennlp.common.params.Params object at 0x7fa1091ac9e8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.is_rhyme(\"корова\", \"здорова\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.is_rhyme('привет', 'ответ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.is_rhyme('пизда', 'джигурда')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transitions import Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(model, prompt, length:int, num_samples:int, allow_linebreak:bool):\n",
    "    logger.info(\"*\" * 200)\n",
    "    logger.info(prompt)\n",
    "   \n",
    "    filter_n = tokenizer.encode('\\n')[-1:]\n",
    "    filter_single = [1] \n",
    "    filter_single += [] if allow_linebreak else filter_n\n",
    "\n",
    "    context_tokens = tokenizer.encode(prompt)\n",
    "    out = sample_sequence(\n",
    "        model=model,\n",
    "        context=context_tokens,\n",
    "        length=length,\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=0.9,\n",
    "        device=device,\n",
    "        filter_single=filter_single,\n",
    "        filter_double=filter_n,\n",
    "        num_samples=num_samples,\n",
    "    ).to('cpu')\n",
    "\n",
    "    prompt = tokenizer.decode(context_tokens)\n",
    "    len_prompt = len(prompt)\n",
    "   \n",
    "    replies = [out[item, :].tolist() for item in range(len(out))]\n",
    "    text = [tokenizer.decode(item)[len_prompt:] for item in replies]\n",
    "    reg_text = [re.match(r'[\\w\\W]*[\\.!?]\\n', item) for item in text]\n",
    "    result = [reg_item[0] if reg_item else item  for reg_item, item in zip(reg_text,text)]\n",
    "    logger.info(\"=\" * 200)\n",
    "    logger.info(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from transformers import (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, \n",
    "                                    GPT2LMHeadModel, GPT2Tokenizer, \n",
    "                                    OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, \n",
    "                                    XLNetLMHeadModel, XLNetTokenizer, \n",
    "                                    TransfoXLLMHeadModel, TransfoXLTokenizer, )\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'xlnet': (XLNetLMHeadModel, XLNetTokenizer),\n",
    "    'transfo-xl': (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "}\n",
    "\n",
    "FILTER_VALUE=-float('Inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = poetry_model\n",
    "length = 150\n",
    "prompt = 'мы с иваном ильичем ехали на дизеле\\nон мудак и я мудак\\n'\n",
    "context = tokenizer.encode(prompt)\n",
    "num_samples = 1\n",
    "temperature=1\n",
    "top_k=0\n",
    "top_p=0.0\n",
    "is_xlnet=False\n",
    "device='cuda'\n",
    "max_input=1023\n",
    "filter_single=[]\n",
    "filter_double=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '\\n' not in prompt:\n",
    "    initial = 'firstline'\n",
    "else:\n",
    "    initial = 'midline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['firstline', 'midline', 'rhymed', 'del']\n",
    "\n",
    "transitions = [\n",
    "    {'trigger': 'newline', 'source': 'firstline', 'dest': 'midline'},\n",
    "    {'trigger': 'newline', 'source': 'midline',   'dest': 'del'},\n",
    "    {'trigger': 'newline', 'source': 'rhymed',    'dest': 'midline'},\n",
    "    {'trigger': 'rhyme',   'source': 'firstline', 'dest': 'del'},\n",
    "    {'trigger': 'rhyme',   'source': 'midline',   'dest': 'rhymed'},\n",
    "    {'trigger': 'rhyme',   'source': 'rhymed',    'dest': 'del'},\n",
    "    {'trigger': 'word',    'source': 'firstline', 'dest': 'firstline'},\n",
    "    {'trigger': 'word',    'source': 'rhymed',    'dest': 'del'},\n",
    "    {'trigger': 'word',    'source': 'midline',   'dest': 'midline'},\n",
    "    \n",
    "    {'trigger': 'newline', 'source': 'del', 'dest': 'del'},\n",
    "    {'trigger': 'rhyme',   'source': 'del', 'dest': 'del'},\n",
    "    {'trigger': 'word',    'source': 'del', 'dest': 'del'},\n",
    "]\n",
    "\n",
    "class Stih(object):\n",
    "    def __init__(self, text=''):\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        self.text = text\n",
    "        self.p = 1\n",
    "        \n",
    "        if '\\n' not in text:\n",
    "            initial = 'firstline'\n",
    "        else:\n",
    "            initial = 'midline'\n",
    "        self.machine = Machine(model=self, states=states, transitions=transitions, initial=initial)\n",
    "    \n",
    "    def add(self, token, p):\n",
    "        token = int(token)\n",
    "        self.tokens += [token]\n",
    "        self.last = tokenizer.decode([token])\n",
    "        self.text = tokenizer.decode(self.tokens)\n",
    "        self.p *= p\n",
    "        \n",
    "        if '\\n' in self.last:\n",
    "            self.newline()\n",
    "            \n",
    "        if re.search(r'(\\w)', self.last):\n",
    "            self.word()\n",
    "        return self.last\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stih = Stih(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rhyme(stih, filtered_logits):\n",
    "    def check_rhyme(token):\n",
    "        last = tokenizer.decode([int(token)])\n",
    "        if not re.search(r'([а-яА-Я])', last):\n",
    "            return False\n",
    "\n",
    "        total_stih = tokenizer.decode(stih.tokens + [token])\n",
    "        last_word = re.findall(r'\\b\\w+\\b', total_stih)[-1] \n",
    "        lines = total_stih.splitlines()\n",
    "        rhyme_line = lines[max(-3, -len(lines))]\n",
    "        rhyme_word = re.findall(r'\\b\\w+\\b', rhyme_line)[-1]\n",
    "        #print(last_word, rhyme_word)\n",
    "        return engine.is_rhyme(last_word, rhyme_word)  \n",
    "    \n",
    "    arr = np.array(np.argsort(filtered_logits.cpu()))[::-1][:6400]\n",
    "    rhymes = Pool(64).map(check_rhyme, arr)\n",
    "    \n",
    "    for itoken in arr:\n",
    "        if filtered_logits[itoken] > FILTER_VALUE:\n",
    "            if rhymes[itoken]:\n",
    "                return itoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mezmorize import Cache\n",
    "\n",
    "cache = Cache(CACHE_TYPE='filesystem', CACHE_DIR='cache')\n",
    "\n",
    "@cache.memoize()\n",
    "def is_rhyme(word1, word2):\n",
    "    #return engine.is_rhyme(word1, word2) \n",
    "    markup_word1 = engine.get_markup(word1).lines[0].words[0]\n",
    "    markup_word1.set_stresses(engine.get_stresses(word1))\n",
    "    markup_word2 = engine.get_markup(word2).lines[0].words[0]\n",
    "    markup_word2.set_stresses(engine.get_stresses(word2))\n",
    "    return Rhymes.is_rhyme(markup_word1, markup_word2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rhyme(stih, token):\n",
    "    last = tokenizer.decode([int(token)])\n",
    "    if not re.search(r'([а-яА-Я])', last):\n",
    "        return False\n",
    "\n",
    "    total_stih = tokenizer.decode(stih.tokens + [token])\n",
    "    last_word = re.findall(r'\\b\\w+\\b', total_stih)[-1] \n",
    "    lines = total_stih.splitlines()\n",
    "    rhyme_line = lines[max(-3, -len(lines))]\n",
    "    rhyme_word = re.findall(r'\\b\\w+\\b', rhyme_line)[-1]\n",
    "    if rhyme_word == last_word: return False\n",
    "    \n",
    "    is_rhymed = is_rhyme(last_word, rhyme_word) \n",
    "    if is_rhymed: print(last_word, rhyme_word)\n",
    "    return is_rhymed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from profilehooks import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profile(immediate=True)\n",
    "def filter_rhyme(stih, filtered_logits):\n",
    "    for itoken in np.array(np.argsort(filtered_logits.cpu()))[::-1][:64000]:\n",
    "        if filtered_logits[itoken] > 0:\n",
    "            if check_rhyme(stih, itoken):\n",
    "                return itoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_after_rhyme(stih, filtered_logits):\n",
    "    for itoken in np.array(np.argsort(filtered_logits.cpu()))[::-1][:64000]:\n",
    "        if filtered_logits[itoken] > 0:\n",
    "            if not re.search(r'(\\w)', tokenizer.decode([int(itoken)])):\n",
    "                return itoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мы с иваном ильичем ехали на дизеле\\nон мудак и я мудак\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stih.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]12/22/2019 16:55:08 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 16:55:08 - INFO - transitions.core -   Entered state midline\n",
      "12/22/2019 16:55:08 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 16:55:08 - INFO - transitions.core -   Entered state rhymed\n",
      "  1%|          | 1/150 [03:28<8:37:37, 208.44s/it]12/22/2019 16:58:37 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 16:58:37 - INFO - transitions.core -   Entered state midline\n",
      "12/22/2019 16:58:37 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 16:58:37 - INFO - transitions.core -   Entered state rhymed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[мы с иваном ильичем ехали на дизеле\n",
      "он мудак и я мудак\n",
      "Идет]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [06:18<8:05:59, 197.02s/it]12/22/2019 17:01:27 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 17:01:27 - INFO - transitions.core -   Entered state midline\n",
      "12/22/2019 17:01:27 - INFO - transitions.core -   Exited state midline\n",
      "12/22/2019 17:01:27 - INFO - transitions.core -   Entered state rhymed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[мы с иваном ильичем ехали на дизеле\n",
      "он мудак и я мудак\n",
      "Идет полдень]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [08:22<10:19:35, 251.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-2896ac466ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstih2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhyme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstih2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'del'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mrhymed_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_rhyme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstih2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhymed_logit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrhymed_logit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-30ca2b606f6a>\u001b[0m in \u001b[0;36mfilter_rhyme\u001b[0;34m(stih, filtered_logits)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfiltered_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_rhyme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mitoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-ebb4c8a3f851>\u001b[0m in \u001b[0;36mcheck_rhyme\u001b[0;34m(stih, token)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrhyme_word\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlast_word\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mis_rhymed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_rhyme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhyme_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_rhymed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhyme_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mis_rhymed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/mezmorize/__init__.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                     \u001b[0mckwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'timeout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_timeout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-22f0332f977a>\u001b[0m in \u001b[0;36mis_rhyme\u001b[0;34m(word1, word2)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_rhyme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#return engine.is_rhyme(word1, word2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmarkup_word1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_markup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmarkup_word1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_stresses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stresses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmarkup_word2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_markup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/rupo/api.py\u001b[0m in \u001b[0;36mget_markup\u001b[0;34m(self, text, language)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mего\u001b[0m \u001b[0mразметка\u001b[0m \u001b[0mпо\u001b[0m \u001b[0mсловарю\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\"\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stress_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_improved_markup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ru\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassificationResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/rupo/util/timeit.py\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s %2.2f sec'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/rupo/main/markup.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(text, stress_predictor)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin_line\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_line\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Проставляем ударения.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mstresses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstress_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;31m# Сопоставляем ударения слогам.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyllables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/rupo/stress/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mstresses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstresses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstresses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/russ/stress/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, word, schema)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPredictSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONSTRAINED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_words_stresses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/russ/stress/model.py\u001b[0m in \u001b[0;36mpredict_words_stresses\u001b[0;34m(self, words, schema)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStressPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_batch_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJsonDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJsonDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_json_to_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJsonDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_batch_instance\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJsonDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0minstance_separated_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/russ/stress/models/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, tags)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embeddings_dropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mencoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder_dropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/allennlp/modules/seq2seq_encoders/pytorch_seq2seq_wrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_and_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0munpacked_sequence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/allennlp/modules/encoder_base.py\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[0;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Actually call the module on the sorted PackedSequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda/envs/gpt/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 529\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    stihi = [stih]\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "            context = torch.tensor([stih.tokens for stih in stihi], dtype=torch.long, device=device)\n",
    "            inputs = {'input_ids': context[:,-max_input:]}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_tokens = torch.zeros(num_samples, dtype=torch.long).to(device)\n",
    "            n_stihi = []\n",
    "            for isample in range(len(stihi)):\n",
    "                next_token_logits = outputs[0][isample, -1, :] / temperature\n",
    "\n",
    "                next_token_logits[filter_single] = FILTER_VALUE\n",
    "                # filter blank line = double \\n\n",
    "                if context[isample, -1] in filter_double:\n",
    "                    next_token_logits[generated[isample, -1]] = FILTER_VALUE\n",
    "\n",
    "                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "                stih1 = copy.deepcopy(stihi[isample])\n",
    "                \n",
    "                if stih1.state != 'rhymed':\n",
    "                    i_logit = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                else:\n",
    "                    i_logit = filter_after_rhyme(stih, filtered_logits)\n",
    "                i_logit = int(i_logit)\n",
    "                stih1.add(i_logit, filtered_logits[i_logit])\n",
    "                n_stihi += [stih1]\n",
    "                \n",
    "                stih2 = copy.deepcopy(stihi[isample])\n",
    "                stih2.rhyme()\n",
    "                if stih2.state != 'del':\n",
    "                    rhymed_logit = filter_rhyme(stih2, filtered_logits)\n",
    "                    print(rhymed_logit)\n",
    "                    if rhymed_logit:\n",
    "                        stih2.add(rhymed_logit, filtered_logits[rhymed_logit])\n",
    "                        stih2.machine.set_state('rhymed')\n",
    "                        n_stihi += [stih2]\n",
    "            stihi = [stih for stih in n_stihi if stih.state != 'del']\n",
    "            stihi = sorted(stihi, key=lambda stih: stih.p, reverse=True)[:64]\n",
    "            print(stihi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мы с иваном ильичем ехали на дизеле\\nКак я тебя любил!\\n'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_stihi[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мы с иваном ильичем ехали на дизеле\n",
      "Горе, гнев, тревога, гарь,\n",
      "Пока из прорехи распахнутой конторы не выбежал,\n",
      "Средь дыма, пыли, грохота, вдох,\n",
      "И, бодрый не меньшеприбыльный, человек\n",
      "В кузове грузовика им. Жуковского одного.\n",
      "\n",
      "Молчаливый, упрямый,\n",
      "Он любил с детства беседы с природой.\n",
      "Сколько тонительных напитков,\n",
      "Сколько красного вина,\n",
      "Сколько винных капелек оказалось -\n",
      "Создателем не предназначено.\n",
      "Сколько буйных чувств! Сколько соков.\n",
      "Сколько смеха и слез,\n",
      "Сколько сладкого щебета!\n",
      "Какие красивые краски...\n",
      "Какая прелесть\n",
      "Розовый, фиолетовый, голубой!\n",
      "И мягкая пружина,\n",
      "Ободряющий, щекочущий.\n",
      "Но радость, отрава эта,\n",
      "Как, наверное, горька,\n",
      "Возникла в просто\n"
     ]
    }
   ],
   "source": [
    "print(stih.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мы с иваном ильичем ехали на дизеле с командующим на полуторке.\n",
      "И пригорок и кустик\n",
      "Осенней упорною петропламенкой\n",
      "облиты белизною, тяжелою.\n",
      "Но зато горы та ж мудрель,\n",
      "и Ташкент с их снежными улыбками\n",
      "не забудет педелю беготни\n",
      "солдатиков-сыновей.\n",
      "Сейчас министры ушли,\n",
      "улетели, замаливая\n",
      "Родина при людях и при детях\n",
      "бедами суровыми.\n",
      "Не слышит пыль, им принадлежащую,\n",
      "солдатский разговор солдатский,\n",
      "что в небе много звезд надоевшей.\n",
      "Поняв, что счастье его\n",
      "тоже детское.\n",
      "За такие торжественные штуки\n",
      "смотрят на него в штыки.\n",
      "Ну а молчание за ними\n",
      "так прочно, что, может быть,\n",
      "даже есть какое-над\n",
      "60\n",
      "? съ\n",
      "и…! –!;\n",
      "- –? Иль!ке,ке.\n",
      "...!; в;\n",
      "назад, с в\n",
      "своем\n",
      ";\n",
      "?!\n",
      "в Де и варен\n",
      "во в\n",
      "в и —!й, с\n",
      "и\n",
      "!\n",
      "на!\n",
      "по\n",
      "\n",
      ":\n",
      ";\n",
      "\n",
      "\n",
      "\n",
      "шке, раке\n",
      "Мо\n",
      "\n",
      ";\n",
      "\n",
      "сверка\n",
      "\n",
      "к\n",
      "\n",
      "в\n",
      "\n",
      "\n",
      "... –; «по\n",
      "Ю пло; по!\"?\n",
      "том\n",
      "по:\n",
      "\n",
      "— досте,...!; зеленом \"\n",
      "\n",
      "в\n",
      "в в! оригин с\n",
      "вместе\n",
      "в\n",
      "\n",
      "на\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "всю\n",
      "своем.: —\n",
      "на\n",
      "\n",
      "\n",
      "чке «рез\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated.to('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/21/2019 11:02:19 - INFO - __main__ -   ********************************************************************************************************************************************************************************************************\n",
      "12/21/2019 11:02:19 - INFO - __main__ -   тест\n",
      "100%|██████████| 50/50 [00:01<00:00, 37.81it/s]\n",
      "12/21/2019 11:02:20 - INFO - __main__ -   ========================================================================================================================================================================================================\n",
      "12/21/2019 11:02:20 - INFO - __main__ -   ['ю,\\nПодымать от зерна...\"\\nИ день как в пламя врезался...\\nИстомившись, ко сну...\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ю,\\nПодымать от зерна...\"\\nИ день как в пламя врезался...\\nИстомившись, ко сну...\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(poetry_model, 'тест', 50, 1, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpt]",
   "language": "python",
   "name": "conda-env-gpt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
